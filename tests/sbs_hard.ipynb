{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_list = [\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"mistralai/Ministral-8B-Instruct-2410\",\n",
    "    \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    \"mistralai/Mistral-Small-Instruct-2409\",\n",
    "    \"mistralai/Mathstral-7B-v0.1\",\n",
    "    \n",
    "    \"Qwen/Qwen2.5-32B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-Math-1.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-Math-7B-Instruct\",\n",
    "    \"Qwen/QwQ-32B-Preview\",\n",
    "    \n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    \"microsoft/Phi-3-small-128k-instruct\",\n",
    "    \"microsoft/Phi-3-medium-128k-instruct\",\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    \"microsoft/phi-4\",\n",
    "    \n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \n",
    "    \"google/gemma-2-9b-it\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "\n",
    "    \"AI-MO/NuminaMath-7B-CoT\",\n",
    "\n",
    "    \"internlm/internlm2_5-1_8b-chat\",\n",
    "    \"internlm/internlm2_5-7b-chat\",\n",
    "    \"internlm/internlm2_5-20b-chat\",\n",
    "    \"internlm/internlm2-math-plus-1_8b\",\n",
    "    \"internlm/internlm2-math-plus-20b\",\n",
    "    \"internlm/internlm2-math-plus-7b\",\n",
    "    \n",
    "    \"deepseek-ai/deepseek-math-7b-instruct\",\n",
    "    \"deepseek-ai/deepseek-math-7b-rl\",\n",
    "    \n",
    "    \"PowerInfer/SmallThinker-3B-Preview\",\n",
    "\n",
    "    \"Skywork/Skywork-o1-Open-Llama-3.1-8B\"\n",
    "]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "print(len(model_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define all unique chat templates\n",
    "CHAT_TEMPLATE_FORMATS = {\n",
    "    \"mistral_format\": \"<s>[INST] {system_message}\\n\\n{user_message}[/INST]\",\n",
    "    \n",
    "    \"qwen_format\": \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    \n",
    "    \"phi3mini_format\": \"<|system|>\\n{system_message}<|end|>\\n<|user|>\\n{user_message}<|end|>\\n<|assistant|>\\n\",\n",
    "    \n",
    "    \"phi3small_format\": \"<|endoftext|><|system|>\\n{system_message}<|end|>\\n<|user|>\\n{user_message}<|end|>\\n<|assistant|>\\n\",\n",
    "    \n",
    "    \"phi3medium_format\": \"<|user|>\\n{user_message}<|end|>\\n<|assistant|>\\n\",\n",
    "    \n",
    "    \"phi4_format\": \"<|im_start|>system<|im_sep|>{system_message}<|im_end|><|im_start|>user<|im_sep|>{user_message}<|im_end|><|im_start|>assistant<|im_sep|>\",\n",
    "    \n",
    "    \"llama_format\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    \n",
    "    \"gemma_format\": \"<bos><start_of_turn>user\\n{user_message}<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "    \n",
    "    \"numina_format\": \"### Problem: {user_message}\\n### Solution: \",\n",
    "    \n",
    "    \"internlm_format\": \"<s><|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    \n",
    "    \"deepseek_format\": \"<｜begin▁of▁sentence｜>{system_message}\\n\\nUser: {user_message}\\n\\nAssistant:\"\n",
    "}\n",
    "\n",
    "# Then map each model to its template format\n",
    "MODEL_TO_TEMPLATE = {\n",
    "    # Mistral Family\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\": CHAT_TEMPLATE_FORMATS[\"mistral_format\"],\n",
    "    \"mistralai/Ministral-8B-Instruct-2410\": CHAT_TEMPLATE_FORMATS[\"mistral_format\"],\n",
    "    \"mistralai/Mistral-Nemo-Instruct-2407\": CHAT_TEMPLATE_FORMATS[\"mistral_format\"],\n",
    "    \"mistralai/Mistral-Small-Instruct-2409\": CHAT_TEMPLATE_FORMATS[\"mistral_format\"],\n",
    "    \"mistralai/Mathstral-7B-v0.1\": CHAT_TEMPLATE_FORMATS[\"mistral_format\"],\n",
    "    \n",
    "    # Qwen Family\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\": CHAT_TEMPLATE_FORMATS[\"qwen_format\"],\n",
    "    \"Qwen/Qwen2.5-14B-Instruct\": CHAT_TEMPLATE_FORMATS[\"qwen_format\"],\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\": CHAT_TEMPLATE_FORMATS[\"qwen_format\"],\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": CHAT_TEMPLATE_FORMATS[\"qwen_format\"],\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": CHAT_TEMPLATE_FORMATS[\"qwen_format\"],\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\": CHAT_TEMPLATE_FORMATS[\"qwen_format\"],\n",
    "    \"Qwen/Qwen2.5-Math-7B-Instruct\": CHAT_TEMPLATE_FORMATS[\"qwen_format\"],\n",
    "    \"Qwen/Qwen2.5-Math-1.5B-Instruct\": CHAT_TEMPLATE_FORMATS[\"qwen_format\"],\n",
    "    \"Qwen/QwQ-32B-Preview\": CHAT_TEMPLATE_FORMATS[\"qwen_format\"],\n",
    "    \n",
    "    # Phi Family\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\": CHAT_TEMPLATE_FORMATS[\"phi3mini_format\"],\n",
    "    \"microsoft/Phi-3.5-mini-instruct\": CHAT_TEMPLATE_FORMATS[\"phi3mini_format\"],\n",
    "    \"microsoft/Phi-3-small-128k-instruct\": CHAT_TEMPLATE_FORMATS[\"phi3small_format\"],\n",
    "    \"microsoft/Phi-3-medium-128k-instruct\": CHAT_TEMPLATE_FORMATS[\"phi3medium_format\"],\n",
    "    \"microsoft/phi-4\": CHAT_TEMPLATE_FORMATS[\"phi4_format\"],\n",
    "    \n",
    "    # Llama Family\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": CHAT_TEMPLATE_FORMATS[\"llama_format\"],\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\": CHAT_TEMPLATE_FORMATS[\"llama_format\"],\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\": CHAT_TEMPLATE_FORMATS[\"llama_format\"],\n",
    "    \n",
    "    # Gemma Family\n",
    "    \"google/gemma-2-9b-it\": CHAT_TEMPLATE_FORMATS[\"gemma_format\"],\n",
    "    \"google/gemma-2-2b-it\": CHAT_TEMPLATE_FORMATS[\"gemma_format\"],\n",
    "    \n",
    "    # NuminaMath\n",
    "    \"AI-MO/NuminaMath-7B-CoT\": CHAT_TEMPLATE_FORMATS[\"numina_format\"],\n",
    "    \n",
    "    # InternLM Family\n",
    "    \"internlm/internlm2_5-1_8b-chat\": CHAT_TEMPLATE_FORMATS[\"internlm_format\"],\n",
    "    \"internlm/internlm2_5-7b-chat\": CHAT_TEMPLATE_FORMATS[\"internlm_format\"],\n",
    "    \"internlm/internlm2_5-20b-chat\": CHAT_TEMPLATE_FORMATS[\"internlm_format\"],\n",
    "    \"internlm/internlm2-math-plus-1_8b\": CHAT_TEMPLATE_FORMATS[\"internlm_format\"],\n",
    "    \"internlm/internlm2-math-plus-7b\": CHAT_TEMPLATE_FORMATS[\"internlm_format\"],\n",
    "    \"internlm/internlm2-math-plus-20b\": CHAT_TEMPLATE_FORMATS[\"internlm_format\"],\n",
    "    \n",
    "    # DeepSeek Math\n",
    "    \"deepseek-ai/deepseek-math-7b-instruct\": CHAT_TEMPLATE_FORMATS[\"deepseek_format\"],\n",
    "    \"deepseek-ai/deepseek-math-7b-rl\": CHAT_TEMPLATE_FORMATS[\"deepseek_format\"],\n",
    "    \n",
    "    # SmallThinker\n",
    "    \"PowerInfer/SmallThinker-3B-Preview\": CHAT_TEMPLATE_FORMATS[\"qwen_format\"],  # Uses same format as Qwen\n",
    "    \n",
    "    # Skywork\n",
    "    \"Skywork/Skywork-o1-Open-Llama-3.1-8B\": CHAT_TEMPLATE_FORMATS[\"llama_format\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1\n",
    "with open(\"sbs_hard_q1.txt\", \"a\") as f:\n",
    "    for checkpoint in model_list:\n",
    "        print(\"<<< checkpoint: \", checkpoint)\n",
    "        f.write(\"<<< checkpoint: \" + checkpoint + \"\\n\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            checkpoint,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"cuda\",\n",
    "            torch_dtype=\"auto\"\n",
    "        )\n",
    "        sbs_hard_prompt = \"\"\"Please reason step by step, and put your final answer within \\\\boxed{{}}.\"\"\"\n",
    "        question = \"What is $10.0000198\\\\cdot 5.9999985401\\\\cdot 6.9999852$ to the nearest whole number\"\n",
    "        if \"gemma\" in checkpoint:\n",
    "            prompt = MODEL_TO_TEMPLATE[checkpoint].format(user_message=question)\n",
    "        else:\n",
    "            prompt = MODEL_TO_TEMPLATE[checkpoint].format(system_message=sbs_hard_prompt, user_message=question)\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=4096, do_sample=False)\n",
    "        generated_ids = out[0][len(inputs.input_ids[0]):]\n",
    "        response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        print(\"\\n<<< response:\")\n",
    "        print(response)\n",
    "        print(\"-\" * 100 + \"\\n\")\n",
    "        f.write(response + \"\\n\")\n",
    "        f.write(\"-\" * 100 + \"\\n\\n\")\n",
    "        del tokenizer\n",
    "        del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "with open(\"sbs_hard_q2.txt\", \"w\") as f:\n",
    "    for checkpoint in model_list:\n",
    "        print(\"<<< checkpoint: \", checkpoint)\n",
    "        f.write(\"<<< checkpoint: \" + checkpoint + \"\\n\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            checkpoint,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"cuda\",\n",
    "            torch_dtype=\"auto\"\n",
    "        )\n",
    "        sbs_hard_prompt = \"\"\"Please reason step by step, and put your final answer within \\\\boxed{{}}.\"\"\"\n",
    "        question = \"Find the point of intersection of the line\\n\\\\[\\\\frac{x - 2}{3} = \\\\frac{y + 1}{4} = \\\\frac{z - 2}{12}\\\\]and $x - y + z = 5.$\"\n",
    "        if \"gemma\" in checkpoint:\n",
    "            prompt = MODEL_TO_TEMPLATE[checkpoint].format(user_message=question)\n",
    "        else:\n",
    "            prompt = MODEL_TO_TEMPLATE[checkpoint].format(system_message=sbs_hard_prompt, user_message=question)\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=4096, do_sample=False)\n",
    "        generated_ids = out[0][len(inputs.input_ids[0]):]\n",
    "        response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        print(\"\\n<<< response:\")\n",
    "        print(response)\n",
    "        print(\"-\" * 100 + \"\\n\")\n",
    "        f.write(response + \"\\n\")\n",
    "        f.write(\"-\" * 100 + \"\\n\\n\")\n",
    "        del tokenizer\n",
    "        del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "with open(\"sbs_hard_q3.txt\", \"w\") as f:\n",
    "    for checkpoint in model_list:\n",
    "        print(\"<<< checkpoint: \", checkpoint)\n",
    "        f.write(\"<<< checkpoint: \" + checkpoint + \"\\n\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            checkpoint,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"cuda\",\n",
    "            torch_dtype=\"auto\"\n",
    "        )\n",
    "        sbs_hard_prompt = \"\"\"Please reason step by step, and put your final answer within \\\\boxed{{}}.\"\"\"\n",
    "        question = \"How many prime numbers less than 100 have a units digit of 3?\"\n",
    "        if \"gemma\" in checkpoint:\n",
    "            prompt = MODEL_TO_TEMPLATE[checkpoint].format(user_message=question)\n",
    "        else:\n",
    "            prompt = MODEL_TO_TEMPLATE[checkpoint].format(system_message=sbs_hard_prompt, user_message=question)\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inputs, max_new_tokens=4096, do_sample=False)\n",
    "        generated_ids = out[0][len(inputs.input_ids[0]):]\n",
    "        response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        print(\"\\n<<< response:\")\n",
    "        print(response)\n",
    "        print(\"-\" * 100 + \"\\n\")\n",
    "        f.write(response + \"\\n\")\n",
    "        f.write(\"-\" * 100 + \"\\n\\n\")\n",
    "        del tokenizer\n",
    "        del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step by step hard\n",
    "sbs_hard_prompt = \"\"\"Please reason step by step, and put your final answer within \\\\boxed{{}}.\"\"\"\n",
    "\n",
    "# step by step\n",
    "sbs_prompt = \"\"\"Please reason step by step, and put your final answer within \\\\boxed{{}} when done reasoning or early-stop keyword **Final Answer** appears.\n",
    "\"\"\"\n",
    "\n",
    "# coarse to fine\n",
    "c2f_prompt = \"\"\"Solve the task by following format:\n",
    "**Coarse Reasoning**\n",
    "Short analysis and an answer. Focus on efficiency and simplicity.\n",
    "\n",
    "**Fine Reasoning**\n",
    "Detailed analysis step by step and a refined answer. Focus on accuracy and correctness.\n",
    "\n",
    "**Final Answer**\n",
    "Your final answer within \\\\boxed{{}} when done reasoning or early-stop keyword **Final Answer** appears.\n",
    "\"\"\"\n",
    "\n",
    "# knowledge first\n",
    "kf_prompt = \"\"\"Solve the task by following format:\n",
    "**Knowledge to Use**\n",
    "List theorem and methods that are useful for solving the problem.\n",
    "\n",
    "**Reasoning**\n",
    "Step by step analysis using above knowledge.\n",
    "\n",
    "**Final Answer**\n",
    "Your final answer within \\\\boxed{{}} when done reasoning or early-stop keyword **Final Answer** appears.\n",
    "\"\"\"\n",
    "\n",
    "# answer and verification\n",
    "aav_prompt = \"\"\"Solve the task by following format:\n",
    "**Quick Answer**\n",
    "Give an initial answer based on intuition or quick calculation.\n",
    "\n",
    "**Verification**\n",
    "Give a revised answer through reasoning step by step to correct potential mistakes.\n",
    "\n",
    "**Final Answer**\n",
    "Your final answer within \\\\boxed{{}} when done reasoning or early-stop keyword **Final Answer** appears.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model chat template\n",
    "# system_prompt = \"Please reason step by step, and put your final answer within \\\\boxed{{}}.\"\n",
    "# system_prompt = \"Please reason step by step, and put your final answer within \\\\boxed{{}} when done reasoning or early-stop keyword **Final Answer** appears.\n",
    "system_prompt = \"\"\"Solve the task by following format:\n",
    "**Knowledge to Use**\n",
    "List theorem and methods that are useful for solving the problem.\n",
    "\n",
    "**Reasoning**\n",
    "Step by step analysis using above knowledge.\n",
    "\n",
    "**Final Answer**\n",
    "Your final answer within \\\\boxed{{}} when done reasoning or early-stop keyword **Final Answer** appears.\n",
    "\"\"\"\n",
    "# system_prompt = \"Solve the task by following format:\\n**Coarse Reasoning**\\nShort analysis and an answer. Focus on efficiency and simplicity.\\n\\n**Fine Reasoning**\\nDetailed analysis step by step and a refined answer. Focus on accuracy and correctness.\\n\\n**Final Answer** \\nYour final answer within \\\\boxed{{}} when done reasoning or early-stop keyword **Final Answer** appears.\"\n",
    "\n",
    "question = \"What is $10.0000198\\\\cdot 5.9999985401\\\\cdot 6.9999852$ to the nearest whole number?\" + system_prompt\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "print(type(text))\n",
    "inputs = tokenizer([question], return_tensors=\"pt\").to(model.device)\n",
    "print(tokenizer.chat_template)\n",
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)\n",
    "out = model.generate(**inputs, max_new_tokens=4096, do_sample=False)\n",
    "print(tokenizer.decode(out[0]))\n",
    "# prompt = question + \"\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.\" + \"1. The robe takes 2 bolts of blue fiber.\\n2. It takes half as much white fiber as blue fiber.\" + \"\\n\\nFinal answer within \\\\boxed{{}}:\\n\"\n",
    "# inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "# prompt = question + \"Solve the task through two types of reasoning:\\n1. Coarse-Grained Reasoning: give quick analysis step by step and an answer. Focus on efficiency and simplicity.\\n2. Fine-Grained Reasoning: give detailed analysis step by step and a refined answer. Foucus on accuracy and correctness.\\nPut final answer within \\\\boxed{{}}.\\n\\nOutput format:\\n**Coarse Reasoning**\\n\\n**Fine Reasoning**\\n\\n**Final Answer** within \\\\boxed{{}}\"\n",
    "# inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "# \"To solve this problem, we start by understanding that the robe requires 2 bolts of blue fiber. The problem states that the robe also requires half as much white fiber as blue fiber.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_template = \"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n",
    "input_template += \"<|im_start|>user\\n{user_prompt}<|im_end|>\\n\"\n",
    "input_template += \"<|im_start|>assistant\\n{assistant_prompt}\\n\"\n",
    "\n",
    "step_by_step_hard = \"Please reason step by step, and put your final answer within \\\\boxed{{}}.\"\n",
    "\n",
    "step_by_step = \"Please reason step by step, and put your final answer within \\\\boxed{{}} when done reasoning or early-stop keyword **Final Answer** appears.\"\n",
    "\n",
    "coarse_to_fine = \"Solve the task by following format:\\n**Coarse Reasoning**\\nShort analysis and an answer. Focus on efficiency and simplicity.\\n\\n**Fine Reasoning**\\nDetailed analysis step by step and a refined answer. Focus on accuracy and correctness.\\n\\n**Final Answer** \\nYour final answer within \\\\boxed{{}} when done reasoning or early-stop keyword **Final Answer** appears.\"\n",
    "\n",
    "\n",
    "user_prompt = \"\"\"What is $10.0000198\\\\cdot 5.9999985401\\\\cdot 6.9999852$ to the nearest whole number?\"\"\"\n",
    "\n",
    "full_cot= \"So I have this problem: calculate \\\\(10.0000198 \\\\times 5.9999985401 \\\\times 6.9999852\\\\) and round it to the nearest whole number. At first glance, it looks like a straightforward multiplication followed by rounding, but the numbers are pretty close to each other, which might make things interesting.\\n\\nFirst, I need to multiply these three numbers together. Since they're all decimals, multiplying them directly might be messy with so many decimal places. Maybe there's a smarter way to approach this without getting bogged down in too much calculation.\\n\\nLet me look at each number individually:\\n\\n- \\\\(10.0000198\\\\)\\n\\n- \\\\(5.9999985401\\\\)\\n\\n- \\\\(6.9999852\\\\)\\n\\nI notice that each of these numbers is very close to a certain integer value. For example:\\n\\n- \\\\(10.0000198\\\\) is almost \\\\(10\\\\)\\n\\n- \\\\(5.9999985401\\\\) is almost \\\\(6\\\\)\\n\\n- \\\\(6.9999852\\\\) is almost \\\\(7\\\\)\\n\\nThis makes me think that maybe these numbers are intended to represent integers plus some small fractions, and their product will simplify nicely.\\n\\nLet me try to express each number as the sum of its integer part and a small fractional part:\\n\\n- \\\\(10.0000198 = 10 + 0.0000198\\\\)\\n\\n- \\\\(5.9999985401 = 6 - 0.0000014599\\\\)\\n\\n- \\\\(6.9999852 = 7 - 0.0000148\\\\)\\n\\nWait, actually, let me double-check those fractional parts:\\n\\nFor \\\\(5.9999985401\\\\):\\n\\n\\\\(6 - 5.9999985401 = 0.0000014599\\\\)\\n\\nSimilarly for \\\\(6.9999852\\\\):\\n\\n\\\\(7 - 6.9999852 = 0.0000148\\\\)\\n\\nHmm, my initial assessment was slightly off. So, more accurately:\\n\\n- \\\\(10.0000198 = 10 + 0.0000198\\\\)\\n\\n- \\\\(5.9999985401 = 6 + 0.0000014599\\\\)\\n\\n- \\\\(6.9999852 = 7 - 0.0000148\\\\)\\n\\nNow, if I multiply these together:\\n\\n\\\\((10 + 0.0000198) \\\\times (6 + 0.0000014599) \\\\times (7 - 0.0000148)\\\\)\\n\\nMultiplying sums can be complicated, especially with such small fractional parts. Maybe there's another way to approximate this product.\\n\\nAlternatively, since the numbers are very close to integers, perhaps their product is also close to an integer, and I just need to find out which one it's closest to.\\n\\nLet me consider the product of the first two numbers:\\n\\n\\\\(10.0000198 \\\\times 5.9999985401\\\\)\\n\\nIf I treat these as approximately equal to 10 and 6, respectively, then:\\n\\n\\\\(10 \\\\times 6 = 60\\\\)\\n\\nBut since both numbers are slightly less than 10 and 6, respectively, their product should be slightly less than 60.\\n\\nSimilarly, multiplying by the third number, which is slightly more than 7, would adjust the product accordingly.\\n\\nMaybe I can use differentials to estimate the effect of these small deviations from the integers.\\n\\nLet me recall that for a function \\\\(f(x)\\\\), the differential \\\\(df \\\\approx f'(x) dx\\\\).\\n\\nIn this case, I can consider each number as a variable and see how changes in them affect the product.\\n\\nLet me denote:\\n\\n\\\\(a = 10.0000198\\\\)\\n\\n\\\\(b = 5.9999985401\\\\)\\n\\n\\\\(c = 6.9999852\\\\)\\n\\nThen, the product is \\\\(p = a \\\\times b \\\\times c\\\\)\\n\\nTaking natural logarithm on both sides:\\n\\n\\\\(\\\\ln p = \\\\ln a + \\\\ln b + \\\\ln c\\\\)\\n\\nNow, using Taylor expansion around \\\\(a_0, b_0, c_0\\\\) where \\\\(a_0 = 10\\\\), \\\\(b_0 = 6\\\\), \\\\(c_0 = 7\\\\):\\n\\n\\\\(\\\\ln p \\\\approx (\\\\ln a_0 + \\\\frac{a - a_0}{a_0} \\\\frac{d^2 \\\\ln a}{da^2}) + (\\\\ln b_0 + \\\\frac{b - b_0}{b_0} \\\\frac{d^2 \\\\ln b}{db^2}) + (\\\\ln c_0 + \\\\frac{c - c_0}{c_0} \\\\frac{d^2 \\\\ln c}{dc^2})\\\\)\\n\\nWait, this seems overly complicated. Maybe I should stick to linear approximation.\\n\\nLet me compute the partial derivatives of \\\\(\\\\ln p\\\\) with respect to \\\\(a\\\\), \\\\(b\\\\), and \\\\(c\\\\):\\n\\n\\\\(\\\\frac{\\\\partial \\\\ln p}{\\\\partial a} = \\\\frac{1}{a}\\\\)\\n\\n\\\\(\\\\frac{\\\\partial \\\\ln p}{\\\\partial b} = \\\\frac{1}{b}\\\\)\\n\\n\\\\(\\\\frac{\\\\partial \\\\ln p}{\\\\partial c} = \\\\frac{1}{c}\\\\)\\n\\nThen, the change in \\\\(\\\\ln p\\\\) due to small changes in \\\\(a\\\\), \\\\(b\\\\), and \\\\(c\\\\) is approximately:\\n\\n\\\\(\\\\delta \\\\ln p \\\\approx \\\\frac{1}{a}(a - a_0) + \\\\frac{1}{b}(b - b_0) + \\\\frac{1}{c}(c - c_0)\\\\)\\n\\nPlugging in the values:\\n\\n\\\\(a - a_0 = 10.0000198 - 10 = 0.0000198\\\\)\\n\\n\\\\(b - b_0 = 5.9999985401 - 6 = -0.0000014599\\\\)\\n\\n\\\\(c - c_0 = 6.9999852 - 7 = -0.0000148\\\\)\\n\\nSo,\\n\\n\\\\(\\\\delta \\\\ln p \\\\approx \\\\frac{1}{10}(0.0000198) + \\\\frac{1}{6}(-0.0000014599) + \\\\frac{1}{7}(-0.0000148)\\\\)\\n\\nCalculating each term:\\n\\n\\\\(\\\\frac{1}{10} \\\\times 0.0000198 = 0.00000198\\\\)\\n\\n\\\\(\\\\frac{1}{6} \\\\times (-0.0000014599) = -0.00000024331666...\\\\)\\n\\n\\\\(\\\\frac{1}{7} \\\\times (-0.0000148) = -0.0000021142857142857142...\\\\)\\n\\nAdding them up:\\n\\n\\\\(0.00000198 - 0.00000024331666... - 0.0000021142857142857142... = -0.0000003775923775923775...\\\\)\\n\\nTherefore,\\n\\n\\\\(\\\\ln p \\\\approx \\\\ln(10 \\\\times 6 \\\\times 7) + \\\\delta \\\\ln p\\\\)\\n\\nSince \\\\(10 \\\\times 6 \\\\times 7 = 420\\\\),\\n\\n\\\\(\\\\ln p \\\\approx \\\\ln 420 - 0.0000003775923775923775...\\\\)\\n\\nComputing \\\\(\\\\ln 420\\\\):\\n\\n\\\\(\\\\ln 420 \\\\approx 6.040527779779047\\\\)\\n\\nSo,\\n\\n\\\\(\\\\ln p \\\\approx 6.040527779779047 - 0.0000003775923775923775... \\\\approx 6.040527399999999...\\\\)\\n\\nExponentiating back to get \\\\(p\\\\):\\n\\n\\\\(p \\\\approx e^{6.040527399999999} \\\\approx 420 - 0.0000003775923775923775 \\\\times 604.0527779779047 \\\\approx 420 - 0.000232...\\\\)\\n\\nWait, that doesn't seem right. Let me check my calculations again.\\n\\nActually, \\\\(e^{-x}\\\\) is approximately \\\\(1 - x\\\\) for small \\\\(x\\\\), but here I have \\\\(e^{\\\\ln p}\\\\), which should give me \\\\(p\\\\). But I'm not sure about the exponentiation step.\\n\\nAlternatively, perhaps I should consider that the product is very close to 420, given that the adjustments are negligible.\\n\\nGiven that the adjustments are extremely small, the product should be very close to 420.\\n\\nTo confirm, let's consider the product of the three numbers directly:\\n\\n\\\\(10.0000198 \\\\times 5.9999985401 \\\\times 6.9999852\\\\)\\n\\nUsing a calculator or computing software, we can find the exact product.\\n\\nBut since I don't have access to that right now, I'll proceed with the assumption that the product is very close to 420.\\n\\nGiven that, rounding 420 to the nearest whole number is simply 420.\\n\\nHowever, to ensure accuracy, perhaps I should consider the possible range of the product.\\n\\nThe smallest possible product would occur if all three numbers are at their minimum values:\\n\\n\\\\(10.0000198 \\\\times 5.9999985401 \\\\times 6.9999852\\\\)\\n\\nAnd the largest possible product would be if they are at their maximum values, but since they are all less than their integer counterparts, the product should still be less than 420.\\n\\nWait, no. Actually, since each number is less than its integer counterpart, their product should be less than 420.\\n\\nWait, hold on:\\n\\n- \\\\(10.0000198 < 10\\\\)\\n\\n- \\\\(5.9999985401 < 6\\\\)\\n\\n- \\\\(6.9999852 > 7\\\\)\\n\\nWait, no, \\\\(6.9999852 < 7\\\\), because 6.9999852 is less than 7.\\n\\nWait, is that correct?\\n\\nLet me check:\\n\\n\\\\(6.9999852\\\\) is indeed less than 7, because subtracting any positive number from 7 gives a result greater than 6.9999852.\\n\\nSo, \\\\(6.9999852 < 7\\\\)\\n\\nTherefore, the product should be less than \\\\(10 \\\\times 6 \\\\times 7 = 420\\\\)\\n\\nBut wait, \\\\(10.0000198 > 10\\\\), \\\\(5.9999985401 < 6\\\\), and \\\\(6.9999852 < 7\\\\)\\n\\nSo, the product is greater than \\\\(10 \\\\times 6 \\\\times 7 = 420\\\\), but less than \\\\(11 \\\\times 5 \\\\times 7\\\\), which is \\\\(385\\\\), but that doesn't make sense because \\\\(10 \\\\times 6 \\\\times 7 = 420\\\\), and since each factor is less than its integer counterpart, the product should be less than 420.\\n\\nWait, no, \\\\(10.0000198 > 10\\\\), \\\\(5.9999985401 < 6\\\\), and \\\\(6.9999852 < 7\\\\)\\n\\nSo, the product is greater than \\\\(10 \\\\times 6 \\\\times 7 = 420\\\\), but less than \\\\(11 \\\\times 5 \\\\times 7 = 385\\\\), which is impossible because 420 is already greater than 385.\\n\\nWait, no, \\\\(11 \\\\times 5 \\\\times 7 = 385\\\\), which is less than 420, but our product is greater than 420 because one factor is greater than 10 and the others are less than their integer values.\\n\\nWait, no, \\\\(10.0000198 > 10\\\\), \\\\(5.9999985401 < 6\\\\), and \\\\(6.9999852 < 7\\\\)\\n\\nSo, the product is greater than \\\\(10 \\\\times 6 \\\\times 7 = 420\\\\), but less than \\\\(11 \\\\times 5 \\\\times 7 = 385\\\\), which is impossible.\\n\\nWait, no, \\\\(11 \\\\times 5 \\\\times 7 = 385\\\\), which is less than 420, but our product has one factor greater than 10 and the others less than their integer values, so it should be between 420 and 385, which is impossible.\\n\\nWait, perhaps I made a mistake in assessing the ranges.\\n\\nLet me re-express the numbers:\\n\\n- \\\\(10.0000198 = 10 + 0.0000198\\\\)\\n\\n- \\\\(5.9999985401 = 6 - 0.0000014599\\\\)\\n\\n- \\\\(6.9999852 = 7 - 0.0000148\\\\)\\n\\nSo, the product is:\\n\\n\\\\((10 + 0.0000198) \\\\times (6 - 0.0000014599) \\\\times (7 - 0.0000148)\\\\)\\n\\nExpanding this:\\n\\nFirst, multiply the first two terms:\\n\\n\\\\((10 + 0.0000198)(6 - 0.0000014599) = 10 \\\\times 6 + 10 \\\\times (-0.0000014599) + 0.0000198 \\\\times 6 + 0.0000198 \\\\times (-0.0000014599)\\\\)\\n\\nWhich simplifies to:\\n\\n\\\\(60 - 0.000014599 + 0.0001188 - 0.0000000003\\\\)\\n\\nApproximately:\\n\\n\\\\(60 - 0.000014599 + 0.0001188 - 0.0000000003 \\\\approx 60.0001042007\\\\)\\n\\nNow, multiply this by the third term:\\n\\n\\\\(60.0001042007 \\\\times (7 - 0.0000148) = 60.0001042007 \\\\times 7 - 60.0001042007 \\\\times 0.0000148\\\\)\\n\\nCalculating each part:\\n\\n\\\\(60.0001042007 \\\\times 7 = 420.0007294049\\\\)\\n\\n\\\\(60.0001042007 \\\\times 0.0000148 = 0.0008712007\\\\)\\n\\nSubtracting:\\n\\n\\\\(420.0007294049 - 0.0008712007 \\\\approx 420.0007294049 - 0.0008712007 = 419.9998582042\\\\)\\n\\nSo, the product is approximately 419.9998582042\\n\\nRounding this to the nearest whole number:\\n\\nLooking at the decimal part, 0.999998582042, which is very close to 1, so rounding up to 420.\\n\\nTherefore, the nearest whole number is 420.\\n\\n**Final Answer**\\n\\n\\\\[ \\\\boxed{420} \\\\]<|im_end|>\"\n",
    "\n",
    "ratio = 0.5\n",
    "assistant_cot = \"\"\n",
    "if ratio > 0.0:\n",
    "    assistant_cot = full_cot[:int(len(full_cot) * ratio)]\n",
    "    assistant_cot += \"\\n\\n**Final Answer** \\n\"\n",
    "\n",
    "prompt = input_template.format(system_prompt=coarse_to_fine, user_prompt=user_prompt, assistant_prompt=assistant_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"formatted input prompt:\")\n",
    "print(\"-\" * 100)\n",
    "print(prompt)\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=4096 if ratio < 0.0 else 25\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"response:\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen25_math_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
