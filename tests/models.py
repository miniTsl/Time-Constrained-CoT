from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

checkpoint = "Skywork/Skywork-o1-Open-Llama-3.1-8B"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
# model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map="auto", torch_dtype=torch.bfloat16)

# prompt = [{"role": "user", "content": "how many r in word strawberry\nPlease think step by step."}]
# tokenized_prompt = tokenizer.apply_chat_template(prompt, add_generation_prompt=True, return_dict=True, return_tensors="pt").to(model.device)

question = "ok"
encoded_question = tokenizer.encode(question, return_tensors="pt")
print(encoded_question)
# out = model.generate(encoded_question, max_new_tokens=512, do_sample=False)

# # Decode only the new tokens generated by the model
# generated_tokens = out[0][len(encoded_question):]
# print(tokenizer.decode(generated_tokens))
print(tokenizer.decode(128000))